package e2e

import (
	"context"
	"fmt"
	"os"
	"strings"
	"testing"
	"time"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/wait"
	k8sclient "k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/clientcmd"
	"k8s.io/klog/v2"
	utilpointer "k8s.io/utils/pointer"

	"github.com/openshift/library-go/pkg/operator/events"
	"github.com/openshift/library-go/pkg/operator/resource/resourceapply"
	"github.com/openshift/library-go/pkg/operator/resource/resourceread"

	"github.com/openshift/run-once-duration-override/test/e2e/bindata"
)

const (
	webhookNamespace = "run-once-duration-override"
	webhookName      = "runoncedurationoverride"
)

func TestMain(m *testing.M) {
	if os.Getenv("KUBECONFIG") == "" {
		klog.Errorf("KUBECONFIG environment variable not set")
		os.Exit(1)
	}

	if os.Getenv("IMAGE_FORMAT") == "" {
		klog.Errorf("IMAGE_FORMAT environment variable not set")
		os.Exit(1)
	}

	if os.Getenv("NAMESPACE") == "" {
		klog.Errorf("NAMESPACE environment variable not set")
		os.Exit(1)
	}

	kubeClient := getKubeClientOrDie()

	eventRecorder := events.NewKubeRecorder(kubeClient.CoreV1().Events("default"), "test-e2e", &corev1.ObjectReference{})

	ctx, cancelFnc := context.WithCancel(context.TODO())
	defer cancelFnc()

	assets := []struct {
		path           string
		readerAndApply func(objBytes []byte) error
	}{
		{
			path: "assets/100_namespace.yaml",
			readerAndApply: func(objBytes []byte) error {
				_, _, err := resourceapply.ApplyNamespace(ctx, kubeClient.CoreV1(), eventRecorder, resourceread.ReadNamespaceV1OrDie(objBytes))
				return err
			},
		},
		{
			path: "assets/200_sa.yaml",
			readerAndApply: func(objBytes []byte) error {
				_, _, err := resourceapply.ApplyServiceAccount(ctx, kubeClient.CoreV1(), eventRecorder, resourceread.ReadServiceAccountV1OrDie(objBytes))
				return err
			},
		},
		{
			path: "assets/300_clusterrole_aggregated_apiserver.yaml",
			readerAndApply: func(objBytes []byte) error {
				_, _, err := resourceapply.ApplyClusterRole(ctx, kubeClient.RbacV1(), eventRecorder, resourceread.ReadClusterRoleV1OrDie(objBytes))
				return err
			},
		},
		{
			path: "assets/301_clusterrolebinding_aggregated_apiserver.yaml",
			readerAndApply: func(objBytes []byte) error {
				_, _, err := resourceapply.ApplyClusterRoleBinding(ctx, kubeClient.RbacV1(), eventRecorder, resourceread.ReadClusterRoleBindingV1OrDie(objBytes))
				return err
			},
		},
		{
			path: "assets/302_clusterrolebinding_auth_delegator.yaml",
			readerAndApply: func(objBytes []byte) error {
				_, _, err := resourceapply.ApplyClusterRoleBinding(ctx, kubeClient.RbacV1(), eventRecorder, resourceread.ReadClusterRoleBindingV1OrDie(objBytes))
				return err
			},
		},
		{
			path: "assets/303_rolebinding_auth_reader.yaml",
			readerAndApply: func(objBytes []byte) error {
				_, _, err := resourceapply.ApplyRoleBinding(ctx, kubeClient.RbacV1(), eventRecorder, resourceread.ReadRoleBindingV1OrDie(objBytes))
				return err
			},
		},
		{
			path: "assets/304_scc_hostnetwork_role.yaml",
			readerAndApply: func(objBytes []byte) error {
				_, _, err := resourceapply.ApplyRole(ctx, kubeClient.RbacV1(), eventRecorder, resourceread.ReadRoleV1OrDie(objBytes))
				return err
			},
		},
		{
			path: "assets/305_scc_hostnetwork_rolebinding.yaml",
			readerAndApply: func(objBytes []byte) error {
				_, _, err := resourceapply.ApplyRoleBinding(ctx, kubeClient.RbacV1(), eventRecorder, resourceread.ReadRoleBindingV1OrDie(objBytes))
				return err
			},
		},
		{
			path: "assets/306_anonymous_cr.yaml",
			readerAndApply: func(objBytes []byte) error {
				_, _, err := resourceapply.ApplyClusterRole(ctx, kubeClient.RbacV1(), eventRecorder, resourceread.ReadClusterRoleV1OrDie(objBytes))
				return err
			},
		},
		{
			path: "assets/307_anonymous_crb.yaml",
			readerAndApply: func(objBytes []byte) error {
				_, _, err := resourceapply.ApplyClusterRoleBinding(ctx, kubeClient.RbacV1(), eventRecorder, resourceread.ReadClusterRoleBindingV1OrDie(objBytes))
				return err
			},
		},
		{
			path: "assets/400_secret.yaml",
			readerAndApply: func(objBytes []byte) error {
				_, _, err := resourceapply.ApplySecret(ctx, kubeClient.CoreV1(), eventRecorder, resourceread.ReadSecretV1OrDie(objBytes))
				return err
			},
		},
		{
			path: "assets/500_daemonset.yaml",
			readerAndApply: func(objBytes []byte) error {
				required := resourceread.ReadDaemonSetV1OrDie(objBytes)
				// override the operator image with the one built in the CI

				// E.g. IMAGE_FORMAT=registry.build03.ci.openshift.org/ci-op-52fj47p4/stable:${component}
				registry := strings.Split(os.Getenv("IMAGE_FORMAT"), "/")[0]

				required.Spec.Template.Spec.Containers[0].Image = registry + "/" + os.Getenv("NAMESPACE") + "/pipeline:run-once-duration-override-webhook"
				_, _, err := resourceapply.ApplyDaemonSet(
					ctx,
					kubeClient.AppsV1(),
					eventRecorder,
					required,
					1000, // any random high number
				)
				return err
			},
		},
		{
			path: "assets/600_mutating.yaml",
			readerAndApply: func(objBytes []byte) error {
				_, _, err := resourceapply.ApplyMutatingWebhookConfigurationImproved(ctx, kubeClient.AdmissionregistrationV1(), eventRecorder, resourceread.ReadMutatingWebhookConfigurationV1OrDie(objBytes), resourceapply.NewResourceCache())
				return err
			},
		},
	}

	// create required resources, e.g. namespace, crd, roles
	if err := wait.PollImmediate(1*time.Second, 10*time.Second, func() (bool, error) {
		for _, asset := range assets {
			klog.Infof("Creating %v", asset.path)
			if err := asset.readerAndApply(bindata.MustAsset(asset.path)); err != nil {
				klog.Errorf("Unable to create %v: %v", asset.path, err)
				return false, nil
			}
		}

		return true, nil
	}); err != nil {
		klog.Errorf("Unable to create RODOO resources: %v", err)
		os.Exit(1)
	}

	webhooksRunning := make(map[string]bool) // node, exists
	// Get the number of master nodes
	if err := wait.PollImmediate(5*time.Second, 1*time.Minute, func() (bool, error) {
		klog.Infof("Listing nodes...")
		nodeItems, err := kubeClient.CoreV1().Nodes().List(ctx, metav1.ListOptions{})
		if err != nil {
			klog.Errorf("Unable to list nodes: %v", err)
			return false, nil
		}
		for _, node := range nodeItems.Items {
			if _, exists := node.Labels["node-role.kubernetes.io/master"]; !exists {
				continue
			}
			webhooksRunning[node.Name] = false
		}
		return true, nil
	}); err != nil {
		klog.Errorf("Unable to collect master nodes")
		os.Exit(1)
	}

	// Wait until the secondary scheduler pod is running
	if err := wait.PollImmediate(5*time.Second, 1*time.Minute, func() (bool, error) {
		klog.Infof("Listing pods...")
		podItems, err := kubeClient.CoreV1().Pods(webhookNamespace).List(ctx, metav1.ListOptions{})
		if err != nil {
			klog.Errorf("Unable to list pods: %v", err)
			return false, nil
		}
		for _, pod := range podItems.Items {
			if !strings.HasPrefix(pod.Name, webhookName+"-") {
				continue
			}
			klog.Infof("Checking pod: %v, phase: %v, deletionTS: %v\n", pod.Name, pod.Status.Phase, pod.GetDeletionTimestamp())
			if pod.Status.Phase == corev1.PodRunning && pod.GetDeletionTimestamp() == nil {
				webhooksRunning[pod.Spec.NodeName] = true
			}
		}

		for node, running := range webhooksRunning {
			if !running {
				klog.Infof("Admission scheduled on node %q is not running", node)
				return false, nil
			}
		}

		return true, nil
	}); err != nil {
		klog.Errorf("Unable to wait for all webhook pods running: %v", err)
		os.Exit(1)
	}

	klog.Infof("All daemonset webhooks are running")
	os.Exit(m.Run())
}

func TestMutation(t *testing.T) {
	tests := []struct {
		name                  string
		pod                   *corev1.Pod
		expectedToSet         bool
		activeDeadlineSeconds int64
	}{
		{
			name: "set activedeadlineseconds to 3600",
			pod: &corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Name: "test-mutating-admission-pod",
				},
				Spec: corev1.PodSpec{
					SecurityContext: &corev1.PodSecurityContext{
						RunAsNonRoot: utilpointer.BoolPtr(true),
						SeccompProfile: &corev1.SeccompProfile{
							Type: corev1.SeccompProfileTypeRuntimeDefault,
						},
					},
					Containers: []corev1.Container{{
						SecurityContext: &corev1.SecurityContext{
							AllowPrivilegeEscalation: utilpointer.BoolPtr(false),
							Capabilities: &corev1.Capabilities{
								Drop: []corev1.Capability{
									"ALL",
								},
							},
						},
						Name:            "pause",
						ImagePullPolicy: "Always",
						Image:           "kubernetes/pause",
						Ports:           []corev1.ContainerPort{{ContainerPort: 80}},
					}},
					RestartPolicy: corev1.RestartPolicyNever,
				},
			},
			expectedToSet:         true,
			activeDeadlineSeconds: 3600,
		},
		{
			name: "set min if pod spec also contains activeDeadlineSeconds",
			pod: &corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Name: "test-mutating-admission-pod",
				},
				Spec: corev1.PodSpec{
					SecurityContext: &corev1.PodSecurityContext{
						RunAsNonRoot: utilpointer.BoolPtr(true),
						SeccompProfile: &corev1.SeccompProfile{
							Type: corev1.SeccompProfileTypeRuntimeDefault,
						},
					},
					Containers: []corev1.Container{{
						SecurityContext: &corev1.SecurityContext{
							AllowPrivilegeEscalation: utilpointer.BoolPtr(false),
							Capabilities: &corev1.Capabilities{
								Drop: []corev1.Capability{
									"ALL",
								},
							},
						},
						Name:            "pause",
						ImagePullPolicy: "Always",
						Image:           "kubernetes/pause",
						Ports:           []corev1.ContainerPort{{ContainerPort: 80}},
					}},
					ActiveDeadlineSeconds: utilpointer.Int64(500),
					RestartPolicy:         corev1.RestartPolicyNever,
				},
			},
			expectedToSet:         true,
			activeDeadlineSeconds: 500,
		},
		{
			name: "not set activeDeadlineSeconds because RestartPolicy is Always",
			pod: &corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Name: "test-mutating-admission-pod",
				},
				Spec: corev1.PodSpec{
					SecurityContext: &corev1.PodSecurityContext{
						RunAsNonRoot: utilpointer.BoolPtr(true),
						SeccompProfile: &corev1.SeccompProfile{
							Type: corev1.SeccompProfileTypeRuntimeDefault,
						},
					},
					Containers: []corev1.Container{{
						SecurityContext: &corev1.SecurityContext{
							AllowPrivilegeEscalation: utilpointer.BoolPtr(false),
							Capabilities: &corev1.Capabilities{
								Drop: []corev1.Capability{
									"ALL",
								},
							},
						},
						Name:            "pause",
						ImagePullPolicy: "Always",
						Image:           "kubernetes/pause",
						Ports:           []corev1.ContainerPort{{ContainerPort: 80}},
					}},
					RestartPolicy: corev1.RestartPolicyAlways,
				},
			},
			expectedToSet:         false,
			activeDeadlineSeconds: 3600,
		},
	}

	for itr, test := range tests {
		t.Run(test.name, func(t *testing.T) {
			// runoncedurationoverrides.admission.runoncedurationoverride.openshift.io/enabled: "true"
			ctx, cancelFnc := context.WithCancel(context.TODO())
			defer cancelFnc()

			clientSet := getKubeClientOrDie()

			testNamespace := &corev1.Namespace{
				ObjectMeta: metav1.ObjectMeta{
					Name: fmt.Sprintf("e2e-rodoo-namespace-%d", itr),
					Labels: map[string]string{
						"runoncedurationoverrides.admission.runoncedurationoverride.openshift.io/enabled": "true",
					},
				},
			}
			if _, err := clientSet.CoreV1().Namespaces().Create(ctx, testNamespace, metav1.CreateOptions{}); err != nil {
				t.Fatalf("Unable to create ns %v err: %v", testNamespace.Name, err)
			}
			defer clientSet.CoreV1().Namespaces().Delete(ctx, testNamespace.Name, metav1.DeleteOptions{})
			test.pod.Namespace = testNamespace.Name

			if _, err := clientSet.CoreV1().Pods(test.pod.Namespace).Create(ctx, test.pod, metav1.CreateOptions{}); err != nil {
				t.Fatalf("Unable to create a pod: %v", err)
			}

			if err := wait.PollImmediate(1*time.Second, time.Minute, func() (bool, error) {
				klog.Infof("Listing pods...")
				pod, err := clientSet.CoreV1().Pods(test.pod.Namespace).Get(ctx, test.pod.Name, metav1.GetOptions{})
				if err != nil {
					klog.Errorf("Unable to get pod: %v", err)
					return false, nil
				}
				if pod.Spec.NodeName == "" {
					klog.Infof("Pod not yet assigned to a node")
					return false, nil
				}
				klog.Infof("Pod successfully assigned to a node: %v", pod.Spec.NodeName)

				if test.expectedToSet {
					if pod.Spec.ActiveDeadlineSeconds == nil || *pod.Spec.ActiveDeadlineSeconds != test.activeDeadlineSeconds {
						klog.Infof("pod.Spec.ActiveDeadlineSeconds is set to %v, expected %v", *pod.Spec.ActiveDeadlineSeconds, test.activeDeadlineSeconds)
						return false, nil
					}
					klog.Infof("pod.Spec.ActiveDeadlineSeconds = %v", *pod.Spec.ActiveDeadlineSeconds)
				} else {
					if pod.Spec.ActiveDeadlineSeconds != nil && *pod.Spec.ActiveDeadlineSeconds == test.activeDeadlineSeconds {
						klog.Infof("pod.Spec.ActiveDeadlineSeconds is set to %v, even though restartPolicy is Always", test.activeDeadlineSeconds)
						return false, nil
					}
				}

				return true, nil
			}); err != nil {
				t.Fatalf("Unable to wait for a scheduled pod: %v", err)
			}
		})
	}
}

func getKubeClientOrDie() *k8sclient.Clientset {
	kubeconfig := os.Getenv("KUBECONFIG")
	config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
	if err != nil {
		klog.Errorf("Unable to build config: %v", err)
		os.Exit(1)
	}
	client, err := k8sclient.NewForConfig(config)
	if err != nil {
		klog.Errorf("Unable to build client: %v", err)
		os.Exit(1)
	}
	return client
}
